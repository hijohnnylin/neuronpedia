#### neuronpedia üß†üîç autointerp server

- [repo status](#repo-status)
- [what this is](#what-this-is)
- [simple non-docker setup](#simple-non-docker-setup)
- [Simple Usage](#simple-usage)
- [some docker commands for reference](#some-docker-commands-for-reference)
- [Testing, Linting, and Formatting](#testing-linting-and-formatting)

## repo status

- we haven't had as much time to work on this, but we'd like to collaborate with eleuther to add more explainers, scorers, and include the openai auto-interp types.
- it would be fantastic to standardize on auto-interp formats (eg an `explainerType` should have xyz fields, a `scorerType` should have abc fields, etc)

## what this is

auto-interp explanations and scoring, using eleutherAI's [delphi](https://github.com/EleutherAI/delphi) (formerly `sae-auto-interp`)

as much as possible we try to use classes/types from the `packages/python/neuronpedia-autointerp-client`, which is autogenerated from the openapi spec under `openapi/schemas/openapi/autointerp-server.yaml`

> ‚ö†Ô∏è **warning:** this is draft documentation. we expect to either have better readmes or use a hosted documentation website.

> ‚ö†Ô∏è **warning:** the eleuther embedding scorer uses an embedding model only supported on CUDA (it won't work on mac mps or cpu)

## simple non-docker setup

1. `poetry lock && poetry install`

2. launch local server

   ```
   # no auto-reload
   poetry run uvicorn server:app --host 0.0.0.0 --port 5003 --workers 1
   # with auto-reload
   poetry run uvicorn server:app --host 0.0.0.0 --port 5003 --workers 1 --reload
   ```

## Simple Usage

Generate an explanation given 2 activations

```
curl -X POST "http://localhost:5003/v1/explain/default" \
  -H "Content-Type: application/json" \
  -d '{
    "activations": [
      {
        "tokens": ["The", " cat", " sat", " on", " the", " mat"],
        "values": [0.0, 0.8, 0.0, 0.0, 0.0, 0.0]
      },
      {
        "tokens": ["I", " like", " felines"],
        "values": [0, 0, 0.9]
      }
    ],
    "openrouter_key": "YOUR_OPENROUTER_KEY",
    "model": "openai/gpt-4o-mini"
  }'
```

See other endpoints under `/schemas/openapi/autointerp/paths`.

## some docker commands for reference

build the image from root directory

```
# cpu
docker build --platform=linux/amd64 -t neuronpedia-autointerp:cpu -f apps/autointerp/Dockerfile --build-arg BUILD_TYPE=nocuda .

# gpu
docker build --platform=linux/amd64 -t neuronpedia-autointerp:gpu -f apps/autointerp/Dockerfile --build-arg BUILD_TYPE=cuda .
```

Push the image to the registry (using google cloud here)

```
# tag + push cpu
docker tag neuronpedia-autointerp:cpu gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:cpu
docker push gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:cpu

# tag + push gpu
docker tag neuronpedia-autointerp:gpu gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:gpu
docker push gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:gpu
```

Run the container locally

```
# replace SECRET with your own value - it must match what webapp knows, or auth will fail

# cpu
docker run -p 5003:5003 -e SECRET=[SECRET] neuronpedia-autointerp:cpu

# gpu
docker run --gpus all -p 5003:5003 -e SECRET=[SECRET] neuronpedia-autointerp:gpu
```

## Testing, Linting, and Formatting

This project uses [pytest](https://docs.pytest.org/en/stable/) for testing, [pyright](https://github.com/microsoft/pyright) for type-checking, and [Ruff](https://docs.astral.sh/ruff/) for formatting and linting.

If you add new code, it would be greatly appreciated if you could add tests in the `tests` directory. You can run the tests with:

```bash
make test
```

Before committing, make sure you format the code with:

```bash
make format
```

Finally, run all CI checks locally with:

```bash
make check-ci
```

If these pass, you're good to go! Open a pull request with your changes.
