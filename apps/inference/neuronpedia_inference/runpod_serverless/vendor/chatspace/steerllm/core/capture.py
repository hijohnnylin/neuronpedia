"""Activation capture handles and response types."""

from __future__ import annotations

from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, Awaitable, Callable

import torch

if TYPE_CHECKING:
    from multiprocessing.shared_memory import SharedMemory


@dataclass
class MessageBoundary:
    """Token boundary information for a message in a chat conversation.

    Attributes
    ----------
    role : str
        Message role (e.g., "system", "user", "assistant").
    content : str
        Original message content.
    start_token : int
        Starting token index in the formatted prompt (inclusive).
    end_token : int
        Ending token index in the formatted prompt (exclusive).
    """

    role: str
    content: str
    start_token: int
    end_token: int

    @property
    def num_tokens(self) -> int:
        """Number of tokens in this message."""
        return self.end_token - self.start_token


@dataclass
class ChatResponse:
    """Response from chat() API with prefill/generated separation.

    When using assistant response prefilling (continue_final_message=True),
    this separates what was prefilled vs. generated by the model.

    Attributes
    ----------
    prefill : str
        Text that was prefilled (from partial assistant message).
        Empty string if no prefill was used.
    generated : str
        Text generated by the model.
    """

    prefill: str
    generated: str

    def full_text(self) -> str:
        """Return complete response text (prefill + generated)."""
        return self.prefill + self.generated

    def __str__(self) -> str:
        """String representation is the full text."""
        return self.full_text()

    @property
    def has_prefill(self) -> bool:
        """Whether this response includes a prefill."""
        return len(self.prefill) > 0

    def to_message(self, role: str = "assistant") -> dict[str, str]:
        """Convert to OpenAI-style message dict."""
        return {"role": role, "content": self.full_text()}


class CaptureHandle:
    """Handle for lazily fetching activation captures for a request.

    Supports async context manager for automatic cleanup of shared memory.

    Usage
    -----
    Recommended: async context manager::

        async with handle:
            captures = handle.captures
            layer_5 = captures[5][0]["hidden"]
        # Shared memory automatically released

    Manual cleanup::

        captures = handle.captures
        await handle.close()

    Attributes
    ----------
    request_id : str
        Internal request identifier for fetching captures.
    layer_indices : tuple[int, ...]
        Layer indices that were captured.
    message_boundaries : tuple[MessageBoundary, ...] | None
        Message boundaries for chat-style captures.
    """

    def __init__(
        self,
        request_id: str,
        layer_indices: tuple[int, ...],
        fetch_fn: Callable[[], Awaitable[dict[int, list[dict[str, Any]]]]],
        cleanup_fn: Callable[[], Awaitable[None]] | None = None,
        message_boundaries: tuple[MessageBoundary, ...] | None = None,
    ) -> None:
        """Initialize capture handle.

        Parameters
        ----------
        request_id :
            Identifier for this capture request.
        layer_indices :
            Layer indices that were captured.
        fetch_fn :
            Async function to fetch captures from backend.
        cleanup_fn :
            Optional async function to release resources.
        message_boundaries :
            Optional message boundaries for chat captures.
        """
        self.request_id = request_id
        self.layer_indices = layer_indices
        self.message_boundaries = message_boundaries
        self._fetch_fn = fetch_fn
        self._cleanup_fn = cleanup_fn
        self._captures: dict[int, list[dict[str, Any]]] | None = None
        self._closed = False
        self._accessed = False

        # Backend-specific storage for shared memory objects
        self._shm_objects: list[SharedMemory] = []

    async def __aenter__(self) -> CaptureHandle:
        """Enter async context manager."""
        return self

    async def __aexit__(self, *args: Any) -> None:
        """Exit context manager and release resources."""
        await self.close()

    async def fetch(self) -> dict[int, list[dict[str, Any]]]:
        """Fetch captures from backend (idempotent).

        Returns
        -------
        dict[int, list[dict[str, Any]]]
            Mapping of layer indices to capture entries. Each entry
            contains "hidden" (and optionally "before", "after", "meta").
        """
        if self._captures is None:
            self._captures = await self._fetch_fn()
        return self._captures

    @property
    def captures(self) -> dict[int, list[dict[str, Any]]]:
        """Get captures (must call fetch() first).

        Raises
        ------
        RuntimeError
            If captures haven't been fetched yet.
        """
        if self._captures is None:
            raise RuntimeError(
                f"Captures not fetched yet for request {self.request_id}. "
                "Call: await handle.fetch()"
            )
        self._accessed = True
        return self._captures

    async def close(self) -> None:
        """Explicitly release resources."""
        if self._closed:
            return
        self._closed = True

        if self._cleanup_fn is not None:
            await self._cleanup_fn()

    def get_message_activations(
        self,
        message_idx: int,
        layer_idx: int,
        *,
        include_generated: bool = False,
    ) -> torch.Tensor:
        """Get activations for a specific message from chat captures.

        Parameters
        ----------
        message_idx :
            Index of the message in the conversation.
        layer_idx :
            Layer index to extract activations from.
        include_generated :
            If True and message_idx is the last message, include
            generated tokens. Otherwise, only message content tokens.

        Returns
        -------
        torch.Tensor
            Activations shape [num_tokens, hidden_size].

        Raises
        ------
        RuntimeError
            If captures haven't been fetched or boundaries unavailable.
        ValueError
            If indices are out of range.
        """
        if self._captures is None:
            raise RuntimeError(
                f"Captures not fetched yet for request {self.request_id}. "
                "Call: await handle.fetch()"
            )

        if self.message_boundaries is None:
            raise RuntimeError(
                "Message boundaries not available for this capture. "
                "This handle was not created from a chat() call."
            )

        if message_idx < 0 or message_idx >= len(self.message_boundaries):
            raise ValueError(
                f"message_idx {message_idx} out of range "
                f"[0, {len(self.message_boundaries)})"
            )

        if layer_idx not in self._captures:
            raise ValueError(
                f"layer_idx {layer_idx} not in captured layers: "
                f"{list(self._captures.keys())}"
            )

        full_hidden = self._captures[layer_idx][0]["hidden"]
        boundary = self.message_boundaries[message_idx]
        start = boundary.start_token
        end = boundary.end_token

        if include_generated and message_idx == len(self.message_boundaries) - 1:
            end = full_hidden.shape[0]

        return full_hidden[start:end]
