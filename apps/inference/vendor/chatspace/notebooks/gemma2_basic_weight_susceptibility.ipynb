{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma2-27B Basic Weight Susceptibility Analysis\n",
    "\n",
    "This notebook analyzes how PC vectors from persona subspace data activate downstream neurons differently between Gemma2-27B base and instruct models.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Do PC vectors (especially PC1) show different activation patterns in base vs instruct models?\n",
    "2. Which layers show the strongest weight differences?\n",
    "3. How do PC vectors compare to random baselines?\n",
    "\n",
    "**Approach:**\n",
    "- Load both base and instruct models\n",
    "- Compute weight differences\n",
    "- Load PC vectors from PCA data\n",
    "- Apply layernorm and project through weight matrices\n",
    "- Compute cosine distances between base and instruct activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "# Import from chatspace.analysis\n",
    "from chatspace.analysis import (\n",
    "    load_pca_data,\n",
    "    load_individual_role_vectors,\n",
    "    load_individual_trait_vectors,\n",
    "    extract_pc_components,\n",
    "    normalize_vector,\n",
    "    gemma2_rmsnorm,\n",
    "    compute_cosine_distances_batch,\n",
    "    compute_weight_statistics\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Gemma2-27B Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "  Base: google/gemma-2-27b\n",
      "  Instruct: google/gemma-2-27b-it\n",
      "\n",
      "This will take several minutes...\n",
      "\n",
      "Model config:\n",
      "  Hidden size: 4608\n",
      "  Num layers: 46\n",
      "  Intermediate size: 36864\n",
      "  Attention heads: 32\n",
      "  KV heads: 16\n"
     ]
    }
   ],
   "source": [
    "# Model identifiers\n",
    "base_model_id = \"google/gemma-2-27b\"\n",
    "instruct_model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "print(f\"Loading models...\")\n",
    "print(f\"  Base: {base_model_id}\")\n",
    "print(f\"  Instruct: {instruct_model_id}\")\n",
    "print(f\"\\nThis will take several minutes...\\n\")\n",
    "\n",
    "# Load config\n",
    "config = AutoConfig.from_pretrained(base_model_id)\n",
    "print(f\"Model config:\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Num layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Intermediate size: {config.intermediate_size}\")\n",
    "print(f\"  Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  KV heads: {config.num_key_value_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996904cd8c9f41b693edb07f1cea872d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Base model loaded: 509 parameters\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_state_dict = {k: v.cpu() for k, v in base_model.state_dict().items()}\n",
    "print(f\"âœ“ Base model loaded: {len(base_state_dict)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading instruct model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a215b06adb442d5bb213159eaf52166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Instruct model loaded: 509 parameters\n"
     ]
    }
   ],
   "source": [
    "# Load instruct model\n",
    "print(\"Loading instruct model...\")\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "instruct_state_dict = {k: v.cpu() for k, v in instruct_model.state_dict().items()}\n",
    "print(f\"âœ“ Instruct model loaded: {len(instruct_state_dict)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Weight Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing weight differences...\n",
      "\n",
      "Computed 509 weight differences\n",
      "\n",
      "Sample statistics:\n",
      "  model.embed_tokens.weight: shape=torch.Size([256000, 4608]), norm=26.9186\n",
      "  model.layers.0.self_attn.q_proj.weight: shape=torch.Size([4096, 4608]), norm=18.6097\n",
      "  model.layers.0.self_attn.k_proj.weight: shape=torch.Size([2048, 4608]), norm=5.1893\n",
      "  model.layers.0.self_attn.v_proj.weight: shape=torch.Size([2048, 4608]), norm=5.1516\n",
      "  model.layers.0.self_attn.o_proj.weight: shape=torch.Size([4608, 4096]), norm=2.9110\n"
     ]
    }
   ],
   "source": [
    "# Compute weight differences\n",
    "print(\"Computing weight differences...\")\n",
    "weight_diffs = {}\n",
    "for name in base_state_dict.keys():\n",
    "    if name in instruct_state_dict:\n",
    "        diff = instruct_state_dict[name] - base_state_dict[name]\n",
    "        weight_diffs[name] = diff\n",
    "\n",
    "print(f\"\\nComputed {len(weight_diffs)} weight differences\")\n",
    "print(f\"\\nSample statistics:\")\n",
    "for i, (name, diff) in enumerate(list(weight_diffs.items())[:5]):\n",
    "    print(f\"  {name}: shape={diff.shape}, norm={diff.float().norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-intro",
   "metadata": {},
   "source": [
    "## 3. Load PC Vectors and Semantic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-load-all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data paths...\n",
      "  Roles PCA: True\n",
      "  Traits PCA: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/chatspace/.venv/lib/python3.11/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/root/chatspace/.venv/lib/python3.11/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Loaded PCA data from layer 22\n",
      "  Extracted 10 PCs\n",
      "  Variance explained by first 5: [0.21216452 0.08200892 0.06822039 0.05221459 0.04578365]\n",
      "\n",
      "âœ“ Loaded semantic vectors:\n",
      "  275 role difference vectors\n",
      "  240 trait contrast vectors\n"
     ]
    }
   ],
   "source": [
    "# Path to persona subspace data\n",
    "persona_data_root = Path(\"/workspace/persona-data\")\n",
    "gemma_model_name = \"gemma-2-27b\"\n",
    "\n",
    "roles_pca_dir = persona_data_root / gemma_model_name / \"roles_240\" / \"pca\"\n",
    "traits_pca_dir = persona_data_root / gemma_model_name / \"traits_240\" / \"pca\"\n",
    "roles_vectors_dir = persona_data_root / gemma_model_name / \"roles_240\" / \"vectors\"\n",
    "traits_vectors_dir = persona_data_root / gemma_model_name / \"traits_240\" / \"vectors\"\n",
    "\n",
    "print(f\"Checking data paths...\")\n",
    "print(f\"  Roles PCA: {roles_pca_dir.exists()}\")\n",
    "print(f\"  Traits PCA: {traits_pca_dir.exists()}\")\n",
    "\n",
    "# Load PCA data\n",
    "pca_data, all_pca_files = load_pca_data(roles_pca_dir)\n",
    "pca_layer = pca_data['layer']\n",
    "print(f\"\\nâœ“ Loaded PCA data from layer {pca_layer}\")\n",
    "\n",
    "# Extract ALL PCs (load 10 for flexibility)\n",
    "n_pcs_total = 10\n",
    "pcs_all, variance_all = extract_pc_components(pca_data, n_components=n_pcs_total)\n",
    "print(f\"  Extracted {n_pcs_total} PCs\")\n",
    "print(f\"  Variance explained by first 5: {variance_all[:5]}\")\n",
    "\n",
    "# Load individual role and trait vectors at PCA layer\n",
    "# Uses discriminative defaults:\n",
    "# - Roles: pos_3 - default_1 (difference vectors)\n",
    "# - Traits: pos_neg_50 (precomputed contrast vectors)\n",
    "role_vectors = load_individual_role_vectors(roles_vectors_dir, pca_layer)\n",
    "trait_vectors = load_individual_trait_vectors(traits_vectors_dir, pca_layer)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded semantic vectors:\")\n",
    "print(f\"  {len(role_vectors)} role difference vectors\")\n",
    "print(f\"  {len(trait_vectors)} trait contrast vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Analysis Configuration:\n",
      "  Analyzing 3 PCs: ['PC1', 'PC2', 'PC3']\n",
      "  Context: Â±5 layers around layer 22\n",
      "  Random baseline: 20 vectors\n",
      "  Sampling 5 roles + 5 traits\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Set analysis parameters here\n",
    "# ============================================================================\n",
    "\n",
    "# Which PCs to visualize in plots\n",
    "plot_pcs = [\"PC1\", \"PC2\", \"PC3\"]\n",
    "# plot_pcs = [\"PC1\"]  # Uncomment to focus on PC1 only\n",
    "\n",
    "# How many layers before/after PCA layer to analyze\n",
    "n_layers_context = 5\n",
    "\n",
    "# How many random baseline vectors\n",
    "n_random_baseline = 20\n",
    "\n",
    "# How many role/trait vectors to sample\n",
    "n_sample_roles = 5\n",
    "n_sample_traits = 5\n",
    "\n",
    "print(f\"ðŸ“‹ Analysis Configuration:\")\n",
    "print(f\"  Analyzing {len(plot_pcs)} PCs: {plot_pcs}\")\n",
    "print(f\"  Context: Â±{n_layers_context} layers around layer {pca_layer}\")\n",
    "print(f\"  Random baseline: {n_random_baseline} vectors\")\n",
    "print(f\"  Sampling {n_sample_roles} roles + {n_sample_traits} traits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-compute-intro",
   "metadata": {},
   "source": [
    "## 4. Extract Weights and Compute Cosine Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-extract-weights",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing layers (Â±5 from layer 22):\n",
      "  Upstream: [17, 18, 19, 20, 21]\n",
      "  Downstream: [23, 24, 25, 26, 27]\n",
      "\n",
      "âœ“ Found 50 relevant weight matrices\n",
      "âœ“ Extracted 20 layernorm weights\n"
     ]
    }
   ],
   "source": [
    "# Define layers to analyze (based on config)\n",
    "upstream_layers = range(max(0, pca_layer - n_layers_context), pca_layer)\n",
    "downstream_layers = range(pca_layer + 1, min(pca_layer + n_layers_context + 1, config.num_hidden_layers))\n",
    "analysis_layers = list(upstream_layers) + list(downstream_layers)\n",
    "\n",
    "print(f\"Analyzing layers (Â±{n_layers_context} from layer {pca_layer}):\")\n",
    "print(f\"  Upstream: {list(upstream_layers)}\")\n",
    "print(f\"  Downstream: {list(downstream_layers)}\")\n",
    "\n",
    "# Extract relevant weights\n",
    "target_weight_types = ['up_proj.weight', 'gate_proj.weight', 'q_proj.weight', 'k_proj.weight', 'v_proj.weight']\n",
    "relevant_weights = {}\n",
    "\n",
    "for name, diff in weight_diffs.items():\n",
    "    for layer_num in analysis_layers:\n",
    "        layer_prefix = f\"model.layers.{layer_num}.\"\n",
    "        if name.startswith(layer_prefix):\n",
    "            for weight_type in target_weight_types:\n",
    "                if name.endswith(weight_type):\n",
    "                    relevant_weights[name] = diff\n",
    "                    break\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(relevant_weights)} relevant weight matrices\")\n",
    "\n",
    "# Extract layernorm weights\n",
    "layernorm_weights = {}\n",
    "for layer_num in analysis_layers:\n",
    "    input_ln_name = f\"model.layers.{layer_num}.input_layernorm.weight\"\n",
    "    pre_ffn_ln_name = f\"model.layers.{layer_num}.pre_feedforward_layernorm.weight\"\n",
    "    \n",
    "    if input_ln_name in base_state_dict:\n",
    "        layernorm_weights[f\"{layer_num}.input_layernorm\"] = {\n",
    "            'base': base_state_dict[input_ln_name],\n",
    "            'instruct': instruct_state_dict[input_ln_name]\n",
    "        }\n",
    "    \n",
    "    if pre_ffn_ln_name in base_state_dict:\n",
    "        layernorm_weights[f\"{layer_num}.pre_feedforward_layernorm\"] = {\n",
    "            'base': base_state_dict[pre_ffn_ln_name],\n",
    "            'instruct': instruct_state_dict[pre_ffn_ln_name]\n",
    "        }\n",
    "\n",
    "print(f\"âœ“ Extracted {len(layernorm_weights)} layernorm weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-build-vectors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Prepared 33 test vectors:\n",
      "  3 PCs: ['PC1', 'PC2', 'PC3']\n",
      "  5 roles\n",
      "  5 traits\n",
      "  20 random baseline\n"
     ]
    }
   ],
   "source": [
    "# Build test vector batch\n",
    "all_vectors = []\n",
    "vector_names = []\n",
    "\n",
    "# Add configured PCs\n",
    "for pc_name in plot_pcs:\n",
    "    pc_idx = int(pc_name.replace(\"PC\", \"\")) - 1\n",
    "    all_vectors.append(pcs_all[pc_idx])\n",
    "    vector_names.append(pc_name)\n",
    "\n",
    "# Sample role vectors\n",
    "sample_roles = list(role_vectors.items())[:n_sample_roles]\n",
    "for name, vec in sample_roles:\n",
    "    all_vectors.append(vec)\n",
    "    vector_names.append(f\"role:{name}\")\n",
    "\n",
    "# Sample trait vectors\n",
    "sample_traits = list(trait_vectors.items())[:n_sample_traits]\n",
    "for name, vec in sample_traits:\n",
    "    all_vectors.append(vec)\n",
    "    vector_names.append(f\"trait:{name}\")\n",
    "\n",
    "# Add random baseline\n",
    "torch.manual_seed(42)\n",
    "for i in range(n_random_baseline):\n",
    "    rand_vec = torch.randn(config.hidden_size, dtype=torch.float32)\n",
    "    rand_vec = normalize_vector(rand_vec).to(torch.bfloat16)\n",
    "    all_vectors.append(rand_vec)\n",
    "    vector_names.append(f\"Random{i+1}\")\n",
    "\n",
    "# Stack into batch\n",
    "vectors_batch = torch.stack(all_vectors)\n",
    "print(f\"âœ“ Prepared {len(vector_names)} test vectors:\")\n",
    "print(f\"  {len(plot_pcs)} PCs: {plot_pcs}\")\n",
    "print(f\"  {len(sample_roles)} roles\")\n",
    "print(f\"  {len(sample_traits)} traits\")\n",
    "print(f\"  {n_random_baseline} random baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-compute-distances",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine distances for 50 weight matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing weights:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_cosine_distances_batch() missing 3 required positional arguments: 'weight_instruct', 'ln_weight_base', and 'ln_weight_instruct'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m proj_instruct = normed_instruct @ instruct_weight.T\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Compute cosine distances\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m cosine_dists = \u001b[43mcompute_cosine_distances_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj_instruct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, vec_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vector_names):\n",
      "\u001b[31mTypeError\u001b[39m: compute_cosine_distances_batch() missing 3 required positional arguments: 'weight_instruct', 'ln_weight_base', and 'ln_weight_instruct'"
     ]
    }
   ],
   "source": [
    "# Compute cosine distances for each weight matrix\n",
    "results = []\n",
    "\n",
    "print(f\"Computing cosine distances for {len(relevant_weights)} weight matrices...\")\n",
    "\n",
    "for weight_name, weight_diff in tqdm(relevant_weights.items(), desc=\"Processing weights\"):\n",
    "    # Parse layer and weight type\n",
    "    parts = weight_name.split('.')\n",
    "    layer_num = int(parts[2])\n",
    "    weight_type = parts[-2]  # e.g., 'up_proj', 'gate_proj', etc.\n",
    "    \n",
    "    # Get corresponding weights from base and instruct\n",
    "    base_weight = base_state_dict[weight_name]\n",
    "    instruct_weight = instruct_state_dict[weight_name]\n",
    "    \n",
    "    # Determine appropriate layernorm\n",
    "    if weight_type in ['up_proj', 'gate_proj']:\n",
    "        ln_key = f\"{layer_num}.pre_feedforward_layernorm\"\n",
    "    else:  # attention weights\n",
    "        ln_key = f\"{layer_num}.input_layernorm\"\n",
    "    \n",
    "    if ln_key not in layernorm_weights:\n",
    "        continue\n",
    "    \n",
    "    ln_base = layernorm_weights[ln_key]['base']\n",
    "    ln_instruct = layernorm_weights[ln_key]['instruct']\n",
    "    \n",
    "    # Apply layernorm and compute projections\n",
    "    normed_base = gemma2_rmsnorm(vectors_batch, ln_base)\n",
    "    normed_instruct = gemma2_rmsnorm(vectors_batch, ln_instruct)\n",
    "    \n",
    "    proj_base = normed_base @ base_weight.T\n",
    "    proj_instruct = normed_instruct @ instruct_weight.T\n",
    "    \n",
    "    # Compute cosine distances\n",
    "    cosine_dists = compute_cosine_distances_batch(proj_base, proj_instruct)\n",
    "    \n",
    "    # Store results\n",
    "    for i, vec_name in enumerate(vector_names):\n",
    "        results.append({\n",
    "            'vector': vec_name,\n",
    "            'layer': layer_num,\n",
    "            'weight_type': weight_type,\n",
    "            'cosine_distance': float(cosine_dists[i])\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nâœ“ Computed {len(results_df)} cosine distance measurements\")\n",
    "print(f\"  Shape: {results_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-intro",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mean cosine distance by vector type\n",
    "summary = results_df.groupby('vector')['cosine_distance'].agg(['mean', 'std', 'min', 'max'])\n",
    "summary = summary.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"\\nMean cosine distance by vector (top 10):\")\n",
    "print(summary.head(10))\n",
    "\n",
    "# Compare configured PCs to random baseline\n",
    "pc_means = {pc: results_df[results_df['vector'] == pc]['cosine_distance'].mean() for pc in plot_pcs}\n",
    "random_mean = results_df[results_df['vector'].str.startswith('Random')]['cosine_distance'].mean()\n",
    "\n",
    "print(f\"\\nPC vs Random baseline comparison:\")\n",
    "print(f\"  Random baseline mean: {random_mean:.6f}\")\n",
    "for pc in plot_pcs:\n",
    "    pc_mean = pc_means[pc]\n",
    "    ratio = pc_mean / random_mean\n",
    "    print(f\"  {pc} mean: {pc_mean:.6f} ({ratio:.2f}x random)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Cosine distance by weight type and layer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# By weight type (configured PCs only)\n",
    "pc_data = results_df[results_df['vector'].isin(plot_pcs)]\n",
    "sns.boxplot(data=pc_data, x='weight_type', y='cosine_distance', ax=axes[0])\n",
    "axes[0].set_title(f'Cosine Distance by Weight Type ({\", \".join(plot_pcs)})')\n",
    "axes[0].set_ylabel('Cosine Distance')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By layer (line plot for each configured PC)\n",
    "pc_data_mean = pc_data.groupby(['layer', 'vector'])['cosine_distance'].mean().reset_index()\n",
    "for pc in plot_pcs:\n",
    "    data = pc_data_mean[pc_data_mean['vector'] == pc]\n",
    "    axes[1].plot(data['layer'], data['cosine_distance'], marker='o', label=pc, linewidth=2)\n",
    "\n",
    "axes[1].axvline(pca_layer, color='red', linestyle=':', alpha=0.5, label=f'PCA layer ({pca_layer})')\n",
    "axes[1].set_title('Mean Cosine Distance by Layer')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Cosine Distance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Cosine distances for configured PCs across layers and weight types\n",
    "pivot_data = pc_data.pivot_table(\n",
    "    index='weight_type',\n",
    "    columns='layer',\n",
    "    values='cosine_distance',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.4f', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Cosine Distance'})\n",
    "ax.set_title(f'Mean Cosine Distance: {\", \".join(plot_pcs)} across Layers and Weight Types')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Weight Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## 6. Key Findings\n",
    "\n",
    "This analysis reveals which layers and weight types show the strongest differences between base and instruct models when processing PC vectors from persona subspace.\n",
    "\n",
    "**Interpretation:**\n",
    "- **Higher cosine distances** = Instruction tuning significantly altered how that weight responds to the semantic direction\n",
    "- **Layer patterns** = Shows where in the network instruction tuning has strongest effect\n",
    "- **Weight type differences** = Reveals whether MLP (gate/up) or attention (q/k/v) is more affected\n",
    "- **PC vs Random** = Measures whether PCs are specifically targeted vs general weight changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
